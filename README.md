# openMP-HPC-workshop

## Scope / Alcance
Algo que no sab√≠a de c hasta que tuve que ocupar "{" "}" para hacer un scope o bloque es que si uno tiene una cla√∫sula "if" o "for" sin "{" "}" es tomada solametne para la l√≠nea de abajo.
```c
  for(int i = 0; i < 3; i++)
    printf("üòé\n");
    printf("üé∂\n");
```
```
üòé
üòé
üòé
üé∂
```

```c
  for(int i = 0; i < 3; i++){
    printf("üòé\n");
    printf("üé∂\n");
  }
```

```
üòé
üé∂
üòé
üé∂
üòé
üé∂
```

### Lo mismo aplica para OpenMP
```c
#pragma omp parallel
  #pragma omp single
```
Es equivalente a
```c
#pragma omp parallel
  {
  #pragma omp single
    {
    }
  }
```
A veces es redundante ocupar llaves, por lo que entendiendo esto se puede tener un c√≥digo m√°s legible

## Variable p√∫blica vs Privada (for loop)
#### P√∫blica
```c
  int k;
  #pragma omp parallel shared(k)
  for(k = 0; k < 3; k++){
    printf("for shared %d t: %d\n", k, omp_get_thread_num());
  }
```
Output
```
for shared 0 t: 0
for shared 0 t: 2
for shared 2 t: 2
for shared 0 t: 1
for shared 0 t: 3
for shared 0 t: 6
for shared 0 t: 4
for shared 1 t: 0
for shared 0 t: 7
for shared 0 t: 5
```
Lo que est√° ocurriendo es una **condici√≥n de carrera** dado que al ser una variable compartida por todas las hebras, todas sobreescriben su valor de manera ca√≥tica, por lo que el ciclo se detiene cuando alguna toma el valor de k = 2 y lo suma, hasta que k = 3.
#### Privada
```c
  #pragma omp parallel
  for(int k = 0; k < 3; k++){
    printf("for priv %d t: %d\n", k, omp_get_thread_num());
  }
```
```
for priv 0 t: 4
for priv 1 t: 4
for priv 2 t: 4
for priv 0 t: 5
for priv 1 t: 5
for priv 2 t: 5
for priv 0 t: 0
for priv 1 t: 0
for priv 2 t: 0
for priv 0 t: 7
for priv 1 t: 7
for priv 2 t: 7
for priv 0 t: 1
for priv 1 t: 1
for priv 2 t: 1
for priv 0 t: 6
for priv 1 t: 6
for priv 2 t: 6
for priv 0 t: 2
for priv 1 t: 2
for priv 2 t: 2
for priv 0 t: 3
for priv 1 t: 3
for priv 2 t: 3
```
En este caso se puede observar que cada hebra tiene su propia copia de la variable k, por lo que debe cada una debe hacer el ciclo for es decir que si se crean 4 hebras cada una imprimir√° 3 veces, ser√≠an 12 l√≠neas, en este caso se crean las hebras 0,1,2,3,4,5,6,7 (8 hebras) por lo que se imprimen 24 l√≠neas.

## Single
```c
#pragma omp single
```
El c√≥digo ejecutado es solo ejecutado por 1 hebra (cualquiera).

## ¬øC√≥mo acumular variables?
En vez de hacer critical/atomic o un lock, si se coloca **reduction (+:ac) se acumula el valor de la variable deseada autom√°ticamente
```c
#pragma omp for reduction(+:ac)
```
## Acumulaci√≥n paralela (Barrera impl√≠cita en for y wait/nowait)
```c
  int arr[5] = {1, 2, 3, 4, 5};
  int N = 5; // n¬∞ elements
  int ac = 0;
  int i = 0;
  #pragma omp parallel
  {
    #pragma omp for reduction(+:ac)
    for(i = 0; i < N; i++){
      ac += arr[i];
      printf("for %d\n", i);
    }
    #pragma omp single
    printf("Sum: %d\n", ac);
  }
```
```
for 4
for 3
for 0
for 1
for 2
Sum: 15
```
**#pragma omp for** tiene impl√≠cita una barrera al final de la ejecuci√≥n por lo que si se realia un un **#pragma omp single** al final se obtendr√° el resultado sincronizado por todas las hebras, un **#pragma omp barrier** ser√≠a redundante

## wait vs no wait
Si se coloca nowait, no se sincroniza al finalizar el for, por lo que se obtiene cualquier resultado **(Condici√≥n de carrera)**
```c
#pragma omp for reduction(+:ac) nowait
```

```
for 3
for 2
for 4
Sum: 0
for 1
for 0
```


## Tarea
Bloque independiente de c√≥digo que ser√° ejecutado cuando se pueda, utilizado para dividir el trabajo a elementos no estructurados como listas enlazadas, √°rboles.

### Creaci√≥n de m√∫ltiples tareas
En este caso se crean 10 tareas y cada una imprime la hebra a la que est√° unida/ligada.
```c
  #pragma omp parallel
  #pragma omp single nowait
  {
    for (int i = 0; i < 10; i++) {
      #pragma omp task
      printf("Thread no. %d\n", omp_get_thread_num());
    }
  }
  return 0;
```

```
Thread no. 3
Thread no. 1
Thread no. 3
Thread no. 4
Thread no. 2
Thread no. 6
Thread no. 5
Thread no. 7
Thread no. 1
Thread no. 0
```

## Ejemplo Multiplicaci√≥n matricial paralela
![image](https://github.com/nic0q/openMP-HPC-workshop/assets/91075814/ffec1f19-b88e-4992-acae-c193aea7d28e)
![image](https://github.com/nic0q/openMP-HPC-workshop/assets/91075814/76980012-2756-4ff1-aed9-6c56fe2f6dd7)

- Se puede apreciar que si se colocan matrices peque√±as el tiempo paralelo es mayor dado que la parte paralela es insignificante y se demora m√°s en la comunicaci√≥n entre hebras.
Sin embargo con matrices grandes se obtiene excelentes resultados:

### Multiplicaci√≥n matricial secuencial
```c
double start_s, end_s;
int N = 512;
int* mat_1 = calloc(N * N, sizeof(int));
int* mat_2 = calloc(N * N, sizeof(int));
int* mat_3 = calloc(N * N, sizeof(int));
int* mat_4 = calloc(N * N, sizeof(int));
```
```c
start_s = omp_get_wtime();
  for (int i = 0; i < N; i++) {   // O(N**3)
    for (int j = 0; j < N; j++) {
      sum = 0;
      for (int k = 0; k < N; k++) {
        sum += mat_1[i * N + k] * mat_2[k * N + j];
      }
      mat_3[i * N + j] = sum;
    }
  }
  end_s = omp_get_wtime();
```
### Multiplicaci√≥n matricial paralela
```c
double start_p, end_p;
start_p = omp_get_wtime();
  #pragma omp parallel for
    for (int i = 0; i < N; i++) { 
      for (int j = 0; j < N; j++) {
        int sum = 0;
        for (int k = 0; k < N; k++) {
          sum += mat_1[i*N+k]*mat_2[k*N+j];
        }
        mat_4[i*N+j ] = sum;
      }
    }
  end_p = omp_get_wtime();
```
#### Resultados tiempo(s)
Tama√±o/Algoritmo | Secuencial | Paralelo
--- | --- | --- | 
32x32 | 0.0010 | 0.0013
64x64 | 0.0011 | 0.0013
128x128 | 0.0078 | 0.0029
256x256 | 0.057 | 0.016
512x512 | 0.420 | 0.105
1024x1024 | 3.969 | 1.376
